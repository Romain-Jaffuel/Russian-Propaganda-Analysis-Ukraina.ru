{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03bb8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import argostranslate.package, argostranslate.translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d3bb85d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLES_DIR = \"./datasets_propaganda/train-articles\"\n",
    "LABELS_FILE = \"./datasets_propaganda/train-task2-TC.labels\"\n",
    "\n",
    "TECHNIQUES = [\n",
    "    \"Loaded_Language\", \"Name_Calling,Labeling\", \"Repetition\", \"Exaggeration,Minimization\",\n",
    "    \"Doubt\", \"Appeal_to_fear-prejudice\", \"Flag-Waving\", \"Causal_Oversimplification\",\n",
    "    \"Slogans\", \"Appeal_to_Authority\", \"Black-and-White_Fallacy\",\n",
    "    \"Thought-terminating_Cliches\", \"Whataboutism,Straw_Men,Red_Herring\",\n",
    "    \"Bandwagon,Reductio_ad_Hitlerum\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c0567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 235/371 [41:48<25:02, 11.05s/it]  "
     ]
    }
   ],
   "source": [
    "labels_map = defaultdict(list)\n",
    "\n",
    "for file_path in glob.glob(os.path.join(ARTICLES_DIR, '*.txt')):\n",
    "    art_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    offsets = []\n",
    "    current = 0\n",
    "    for line in lines:\n",
    "        offsets.append((current, current + len(line)))\n",
    "        current += len(line)\n",
    "    with open(LABELS_FILE, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            aid, label, start, end = line.strip().split('\\t')\n",
    "            if aid != art_id:\n",
    "                continue\n",
    "            span_start = int(start)\n",
    "            for idx, (lo, hi) in enumerate(offsets):\n",
    "                if lo <= span_start < hi:\n",
    "                    labels_map[(aid, idx)].append(label)\n",
    "                    break\n",
    "\n",
    "examples = []\n",
    "article_files = glob.glob(os.path.join(ARTICLES_DIR, '*.txt'))\n",
    "\n",
    "for file_path in tqdm(article_files):\n",
    "    art_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        text = line.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        translated = argostranslate.translate.translate(text, \"en\", \"ru\")\n",
    "        key = (art_id, i)\n",
    "        techniques_here = labels_map.get(key, [])\n",
    "        label_vector = [1 if t in techniques_here else 0 for t in TECHNIQUES]\n",
    "        examples.append({\"text\": translated, \"labels\": label_vector})\n",
    "\n",
    "with open(\"semeval_translated_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        json.dump(ex, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Done. {len(examples)} examples saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dad327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/371 [00:24<2:33:16, 24.86s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mru\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m     35\u001b[0m key \u001b[38;5;241m=\u001b[39m (art_id, i)\n\u001b[0;32m     36\u001b[0m techniques_here \u001b[38;5;241m=\u001b[39m labels_map\u001b[38;5;241m.\u001b[39mget(key, [])\n",
      "File \u001b[1;32mc:\\Users\\Andraf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\googletrans\\client.py:219\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[1;34m(self, text, dest, src)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    218\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(resp)\n\u001b[1;32m--> 219\u001b[0m parsed \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# not sure\u001b[39;00m\n\u001b[0;32m    221\u001b[0m should_spacing \u001b[38;5;241m=\u001b[39m parsed[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Andraf\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
     ]
    }
   ],
   "source": [
    "labels_map = defaultdict(list)\n",
    "\n",
    "for file_path in glob.glob(os.path.join(ARTICLES_DIR, '*.txt')):\n",
    "    art_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    offsets = []\n",
    "    current = 0\n",
    "    for line in lines:\n",
    "        offsets.append((current, current + len(line)))\n",
    "        current += len(line)\n",
    "    with open(LABELS_FILE, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            aid, label, start, end = line.strip().split('\\t')\n",
    "            if aid != art_id:\n",
    "                continue\n",
    "            span_start = int(start)\n",
    "            for idx, (lo, hi) in enumerate(offsets):\n",
    "                if lo <= span_start < hi:\n",
    "                    labels_map[(aid, idx)].append(label)\n",
    "                    break\n",
    "\n",
    "examples = []\n",
    "article_files = glob.glob(os.path.join(ARTICLES_DIR, '*.txt'))\n",
    "\n",
    "for file_path in tqdm(article_files):\n",
    "    art_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        text = line.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        translated = translator.translate(text, src=\"en\", dest=\"ru\").text\n",
    "        key = (art_id, i)\n",
    "        techniques_here = labels_map.get(key, [])\n",
    "        label_vector = [1 if t in techniques_here else 0 for t in TECHNIQUES]\n",
    "        examples.append({\"text\": translated, \"labels\": label_vector})\n",
    "\n",
    "with open(\"semeval_translated_dataset.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for ex in examples:\n",
    "        json.dump(ex, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Done. {len(examples)} examples saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
